\documentclass{sigchi}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is      granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}
% Some optional stuff you might like/need.
\usepackage{microtype} % Improved Tracking and Kerning
% \usepackage{hypercap}  % Fixes bug in hyperref caption linking
\usepackage{ccicons}  % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of your draft document, you
% have to enable the "chi_draft" option for the document class. To do this, change the very first
% line to: "\documentclass[chi_draft]{sigchi}". You can then place todo notes by using the "\todo{...}"
% command. Make sure to disable the draft option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Lightweight Document Classification for Device-based APP-Recommendation: A Graph-based Approach}
\def\plainauthor{% ANONYMISEDFirst Author, Second Author, Third Author,
}
\def\emptyauthor{}
\def\plainkeywords{App recommendation; Document classification; Summarization}
\def\plaingeneralterms{Document classification}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{3}
\author{%
% ANONYMISED
%   \alignauthor{1st Author Name\\
%     \affaddr{Affiliation}\\
%     \affaddr{City, Country}\\
%     \email{e-mail address}}\\
%   \alignauthor{2nd Author Name\\
%     \affaddr{Affiliation}\\
%     \affaddr{City, Country}\\
%     \email{e-mail address}}\\
%   \alignauthor{3rd Author Name\\
%     \affaddr{Affiliation}\\
%     \affaddr{City, Country}\\
%     \email{e-mail address}}\\
}

\maketitle

\begin{abstract}
We consider the problem of lift document classification on mobile devices.
Document classification is the task of automatically assigning a set of unlabeled documents into a set of predefined categories.
This technique is relevant for app-recommendation systems on mobile phones. 
While app-stores provide basic recommendation functionality, more advanced recommendation systems require fine-grained usage information available only locally on the mobile device. 
However, due to severe resource restrictions on such devices, computational cost needs to be optimised. 
In this paper, summarization as an approach to circumvent the curse of dimensionality is investigated. 
High dimensional feature space can be reduced significantly by considering summarized document as a feature set, since it includes the most important information of the original document. 
Graph-based summarization technique is applied on the classification process, and remarkably improves the performance of document classification.
\end{abstract}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous} \category{See
  \url{http://acm.org/about/class/1998/} for the full list of ACM
  classifiers. This section is required.}{}{}

\keywords{\plainkeywords}

\section{Introduction}
It has become easy to find an app for virtually any possible category but challenging to identify good and reliable apps from this overwhelming choice. 
Although app-stores typically provide basic recommendation functionality, such systems favor apps with a bigger crowd of users such as corporation-developed apps or older and therefore better known apps.
They can not take into account the individual interest of users and their usage habits. 
This problem has been tackled by app recommendation systems which require local installation~\cite{Yan-mobisys-2011,Shi-sigkdd-2012}.
However, these systems are highly resource demanding and therefore not applicable in practical everyday use.
What is required is a computationally cheap approach that is feasible for the application on end-user mobile devices. 
In this paper, we tackle this problem by considering summarization as an approach to circumvent the curse of dimensionality in document classification.

Automatic document classification (also known as text categorization, or topic spotting) is the task of automatically sorting a set of documents into categories (or classes, or topics) from a predefined set~\cite{Sebastiani:2002:MLA:505282.505283}. 
For app-recommendation systems, document classification is an essential component to group apps into categories according to their textual description, crawled, for instance, from online-appstores. 

In document classification, one document is often represented as a vector of words (bag of words), and all these words are not that informative to be included in the final feature set. 
Therefore feature selection should be applied not only to select the most relevant features, but to reduce the high dimensionality of feature vector space. 
In this paper, text summarization will be considered as a feature selection technique to extract the least number of features with the most informativeness for each category.

Online app-stores and the description of apps therein are subject to constant change. 
Furthermore, a ground truth for correct classification is naturally missing. 
In order to produce comparable results and to reliably measure the performance of our approach, we apply our approach to the Reuters-21578 corpus which is a standard benchmark for document classification. 
It has been employed in multiple scientific publications in many research areas especially in information retrieval, natural language
processing and machine learning. 
The hidden semantic relationship between some categories and the skewed distribution of documents make Reuters-21578 corpus most
interesting for document classification with respect to app recommendation systems~\cite{debole2005analysis}. 
Moreover, it has several categories which own very few positive training examples; challenging the performance of the document classification system based on machine learning methods.
The documents refer to the Reuters newswire in 1987 and the classification was done manually by personnel from Reuters Ltd. 
Due to its large number of categories, different subsets of its categories have been adopted as dataset. 
A subset of~30 categories will be taken into account for this project, with at least one positive training example and one test example.

The rest of this document is orgainzed as follows. 
In Section~\ref{sectionRelatedWork}, related work is reviewed. 
Section~\ref{sectionMethodology} presents our approach.
Section~\ref{sectionExperiments} details our results and section~\ref{sectionConclusion} concludes the discussion.

\section{Related Work}\label{sectionRelatedWork}
Application recommenders have started to become increasingly commonplace, with several academic~\cite{Yan-mobisys-2011,Shi-sigkdd-2012} and commercial systems~\footnote{The Aptoide meta-store: \url{http://m.aptoide.com/}} %% I did not find any. Do we have examples for these?
emerging in recent years. 
Some of these operate as separate applications that are installed on the device, such as Aptoide and Cydia, but some, like Aptoide, can also be reached through a web browser on another device.
%TODO: Actually, I think this is illegal. There's a store in China that swipes apps from Google Play. with the marketplace itself~\cite{?}. %%Here again: Could not find an app to search apps. 
At the same time, recommendations have started to emerge on the marketplaces themselves, e.g. Google Play supports both personalized recommendations and country-specific "featured" and most popular application listings.

First works on mobile app recommendations were focused on adopting standard content-based and collaborative filtering techniques for generating recommendations. 
Most of these works operated directly on the marketplace and relied on application popularity, such as installation counts and click stream data, or ratings to generate recommendations~\cite{Bostrom:2008:CIU:1409240.1409280}. 
However, as shown by Falaki et al.~\cite{Falaki-mobisys-2010}, installation counts are a poor indicator of user interest as users tend to try out applications without necessarily ever using them again.
Moreover, some users may not bother uninstalling apps but rather keep apps that have been tried only once. %%TODO: do we have any figures or reference for this?
These apps will then receive the same weight as those that are used regularly. 
The same holds for ratings which do not necessarily reflect true user interest. 
For example, many users give a one star rating for apps that do not function properly on their device~\cite{Yan-mobisys-2011}. 
At the same time, usage patterns have been shown to be highly contextualized, with many applications only being used in specific contexts~\cite{Bohmer-mobilechi-2011}.

Motivated by the findings of the application usage studies, most recent works on app recommendation rely on usage information gathered directly from the device. 
For example, AppJoy~\cite{Yan-mobisys-2011} considers a weighted model where recency, frequency, and duration of interactions are taken into consideration, whereas GetJar~\cite{Shi-sigkdd-2012} and the Djinn system of Karatzoglou et al.~\cite{Karatzoglou-cikm-2012} consider information derived from binary usage patterns. AppJoy relies on a constantly running background process that monitors app use, while both GetJar and our technique can be used with crowdsourced,
infrequently sampled data.
Both AppJoy and GetJar are based on standard recommender system techniques, whereas Djinn is based on tensor-factorization model that additionally considers also the usage context of applications. 
Also other works on integrating context information as part of app recommendations have been proposed~\cite{Mizzaro-carr-2014, Davidsson-carr-2011, woerndl-2007}. 
Finally, the AppAware system of Girardello and Michahelles~\cite{Girardello-mobilechi-2010} provides a Twitter-style social media based systems where users can receive app recommendations from friends and collectively nearby users. 

%Another issue with these works is that some applications are only used in specific contexts~\cite{}. 

%Application marketplaces, including Google Play, can offer personalized recommendations based on the download of the user and the ratings of the applications. In any case, they do not have information from the real usage of the application: whether the app have been used later than just couple of days after installation, or months later than the positive rating have been given. The users may not actually interact equally with all the applications they have installed~\cite{Falaki-mobisys-2010}.

% Previous extended recommendation systems for mobile applications have based on, for example, ratings or installations~\cite{Shi-sigkdd-2012, Girardello-mobilechi-2010} or user's context information.

% AppJoy~\cite{Yan-mobisys-2011} is an application to monitor actual usage patterns of mobile applications, but it does not perform a long-term analysis of usage patterns between different device models and users. AppJoy had 4600 devices, recommended apps based on frequency of use with decay over time, observed that recommended apps were used longer than spontaneously discovered apps.

Usage trends of mobile applications have been monitored by using download counts~\cite{Petsas-imc-2013} or in case of usage, only focusing on short period of time~\cite{Bohmer-mobilechi-2011}.
Recently, the AppTrends approach proposed to consider actual usage data and to base the recommendation on frequency of co-usage of apps~\cite{7072833}.
The show that recommendations should consider co-usage of apps as particular use cases on mobile devices involve a common set of apps rather than individual apps only. 


As it is mentioned before, feature selection as a fundamental task plays an important role in the overall performance of automatic DC. 
Many techniques and approaches has been studied and deployed in feature selection, which all focus on aggressive dimensionality reduction. 
Apart from common feature selection methods such as Document Frequency (DF), Information Gain (IG), Statistic (CHI) and Mutual Information (MI), in several researches text summarization was applied as feature selection and it was found useful and beneficial in automatic DC.
Ker and Chen~\cite{Ker:2000:TCB:1117755.1117766} proposed a summarization-based document classification system. 
Among Several techniques for text summarization (which includes methods based on position, cue phrase, word frequency and discourse segmentation) word-based frequency and position methods were considered and then combined to extract features. 
From position point of view, title of the document was only used with the assumption that existing words in the title probably describes the context relatively well.
After weighting the selected features, DC process was performed by a probabilistic classifier runs on TF-IDF (Term frequency–Inverse Document Frequency). 
The experiment showed that using title as a summarization technique would result in acceptable performance, meanwhile decline the execution time.

The work by Kolcz et al.~\cite{Kolcz:2001:SFS:502585.502647} tried to prove the efficiency of their proposed summarization technique by using it as a feature selection method in DC. 
Different summarization methods based on the title of the story, paragraphs and best sentence were considered in the approach in order to reduce the feature set to a manageable size. Paragraphs' position, keywords and title words were taken into account in summary generation, which included first paragraph, first two paragraph, first last paragraph, paragraph with most keywords, paragraph with most title words. In addition, another summary was also constructed by choosing the sentences with at least 3 title words and at least 4 keywords. The applied classifier was
Support Vector Machine (SVM). The applied summarization methods were as effective as state-of-art statistical feature selection methods in DC, specially the best sentence-based summary.
In another text summarization method, Ko et al.~\cite{Ko:2004:ITC:975966.975971} determined the importance of sentences in a document by combining two methods. First, instead of directly using terms of the title, the most similar sentences with the title were selected, and then in the second method the sentence with the highest sum of weighted terms (based on TF, IDF, and X2 statistics values). Then, the chosen sentences were scored by a modified weighting technique, to retrieve the most informative sentence. 
The suggested system enhanced the performance of document classification in four applied classifiers: Naive Bayes, Rocchio, k-NN, and SVM classifier, regardless of specific language.
Mihalcea and Hassan~\cite{mihalcea2005using} presented a new approach by summarizing the documents in order to improve and enhance the classifier which results in efficient execution of a DC task.
The extraction of sentences was performed with the aid of graph-based algorithms which ranked them according to the number of links. 
Two popular ranking algorithms: PageRank~cite{Brin20123825} and HITS (Hyperlinked Induced Topic Search)~\cite{Kleinberg:1999:ASH:324133.324140} were deployed in order to decide on the importance of sentences which finally construct the summarization. 
Mapping these algorithms to the document, each sentence is considered as a vertice, and if each pair of sentences have some informative terms in common, then there will be an edge between them. 
Naive Bayes, Rocchio were applied as main classifiers. 
The results revealed that the proposed system improved the classification efficiency, disconsidering the length of the original document. 
The technique was also recommended as a measurement for different summarization tool evaluation.
Jiang et al.~\cite{4797414} examined the impact of summarzation on document classification, by considering two different applications of summary in order to obtain the feature set: considering the summary itself as the feature set and applying classical feature selection method (MI, IG and DF) on summary. To construct the summary, only nouns and verbs were weighted, with regards to semantic-based distance value and connective strength value. 
The calculated weights then were used in determining the constituent sentences of the final summarization. 
The outcome indicated that the approach decreased the classification computations and provided a high speed system with acceptable performance.

The reviewed approaches and methods in this section present different methods of summarization. 
Initial works considered the summary as a part or fraction of the document (title, paragraph, etc.). 
Later works try to generate more precise summaries, using many complicated term-based and sentence-based weighting techniques. 
Graph-based algorithms were also quite successful in creating valuable summaries. 
Many informative features were missing in the simple extraction techniques, for instance title itself cannot be a good candidate for a summary, since there is at least one sentence in a document which is more descriptive. 
On the contrary, complex summarization methods lead in more reliable summaries, since many aspects are taken into account in their process.
However, summarization as a feature selection method is a promising technique to effectively improve the performance of document classification systems.


\section{Methodology}\label{sectionMethodology}
The styles contained in this document have been modified from the
default styles to reflect ACM formatting conventions. For example,
content paragraphs like this one are formatted using the Normal style.

\subsection{Preprocessing}\label{sectionPreprocessing}
Your paper's title, authors and affiliations should run across the
full width of the page in a single column 17.8 cm (7 in.) wide.  The
title should be in Helvetica or Arial 18-point bold.  Authors' names
should be in Times New Roman or Times Roman 12-point bold, and
affiliations in 12-point regular.  

See \texttt{{\textbackslash}author} section of this template for
instructions on how to format the authors. For more than three
authors, you may have to place some address information in a footnote,
or in a named section at the end of your paper. Names may optionally
be placed in a single centered row instead of at the top of each
column. Leave one 10-point line of white space below the last line of
affiliations.

\subsection{Document Summarization}\label{sectionSummarization}

Every submission should begin with an abstract of about 150 words,
followed by a set of Author Keywords and ACM Classification
Keywords. The abstract and keywords should be placed in the left
column of the first page under the left half of the title. The
abstract should be a concise statement of the problem, approach, and
conclusions of the work described. It should clearly state the paper's
contribution to the field of HCI\@.

\subsection{Classification}\label{sectionClassification}

Please use a 10-point Times New Roman or Times Roman font or, if this
is unavailable, another proportional font with serifs, as close as
possible in appearance to Times Roman 10-point. Other than Helvetica
or Arial headings, please use sans-serif or non-proportional fonts
only for special purposes, such as source code text.

\section{Experiments and Results}\label{sectionExperiments}

The heading of a section should be in Helvetica or Arial 9-point bold,
all in capitals. Sections should \textit{not} be numbered.


\section{Conclusion}\label{sectionConclusion}

Place figures and tables at the top or bottom of the appropriate
column or columns, on the same page as the relevant text (see
Figure~\ref{fig:figure1}). A figure or table may extend across both
columns to a maximum width of 17.78 cm (7 in.).

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance{}


% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{documentClassification}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
